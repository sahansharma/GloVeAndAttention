{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector embeddings Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt' \n",
    "# (glove data set from: https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "#filename = 'numberbatch-en.txt'\n",
    "#(https://github.com/commonsense/conceptnet-numberbatch)\n",
    "\n",
    "def loadembeddings(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Word vector embeddings Loaded.')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "\n",
    "# Pre-trained word embedding\n",
    "vocab,embd = loadembeddings(filename)\n",
    "\n",
    "word_vec_dim = len(embd[0]) # word_vec_dim = dimension of each word vectors\n",
    "\n",
    "e = np.zeros((word_vec_dim,),np.float32)+0.0001\n",
    "\n",
    "vocab.append('<UNK>') #<UNK> represents unknown word\n",
    "embdunk = np.asarray(embd[vocab.index('unk')],np.float32)+e\n",
    "    \n",
    "vocab.append('<EOS>') #<EOS> represents end of sentence\n",
    "embdeos = np.asarray(embd[vocab.index('eos')],np.float32)+e\n",
    "\n",
    "vocab.append('<PAD>') #<PAD> represents paddings\n",
    "\n",
    "flag1=0\n",
    "flag2=0\n",
    "\n",
    "for vec in embd:\n",
    "    \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdunk)):\n",
    "        flag1=1\n",
    "        print \"FLAG1\"   \n",
    "    if np.all(np.equal(np.asarray(vec,np.float32),embdeos)):\n",
    "        flag2=1\n",
    "        print \"FLAG2\"\n",
    "\n",
    "if flag1==0:\n",
    "    embd.append(embdunk)  \n",
    "if flag2 == 0:\n",
    "    embd.append(embdeos)  \n",
    "    \n",
    "embdpad = np.zeros(word_vec_dim)\n",
    "embd.append(embdpad)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('<UNK>')]\n",
    "\n",
    "def most_similar_eucli(x):\n",
    "    xminusy = np.subtract(embedding,x)\n",
    "    sq_xminusy = np.square(xminusy)\n",
    "    sum_sq_xminusy = np.sum(sq_xminusy,1)\n",
    "    eucli_dists = np.sqrt(sum_sq_xminusy)\n",
    "    return np.argsort(eucli_dists)\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    most_similars = most_similar_eucli(np.asarray(vec,np.float32))\n",
    "    return vocab[most_similars[0]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open ('AmazonPICKLE', 'rb') as fp:\n",
    "    PICK = pickle.load(fp)\n",
    "\n",
    "vocab_limit = PICK[0]\n",
    "vocab_len = len(vocab_limit)\n",
    "\n",
    "batch_size = int(PICK[1])\n",
    "\n",
    "batches_x = PICK[2]\n",
    "batches_y = PICK[3]\n",
    "\n",
    "batches_x_pe = PICK[4] #already position encoded\n",
    "\n",
    "max_len = len(batches_y[0][0]) #max output len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_limit = []\n",
    "\n",
    "for i in xrange(0,vocab_len):\n",
    "    embd_limit.append(word2vec(vocab_limit[i]))\n",
    "\n",
    "np_embd_limit = np.asarray(embd_limit,dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare training data\n",
    "\n",
    "train_len = int(0.75*len(batches_x))\n",
    "\n",
    "train_batches_x= batches_x[0:train_len]\n",
    "train_batches_x_pe = batches_x_pe[0:train_len]\n",
    "\n",
    "train_batches_y = batches_y[0:train_len]\n",
    "\n",
    "# (Rest of the data can be used for validating and testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Hyperparamters\n",
    "\n",
    "h=8 #no. of heads\n",
    "N=1 #no. of decoder and encoder layers\n",
    "learning_rate=0.001\n",
    "epochs = 200\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#Placeholders\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None,None,word_vec_dim])\n",
    "y = tf.placeholder(tf.int32, [None,None])\n",
    "\n",
    "output_len = tf.placeholder(tf.int32)\n",
    "\n",
    "teacher_forcing = tf.placeholder(tf.bool)\n",
    "\n",
    "tf_pad_mask = tf.placeholder(tf.float32,[None,None])\n",
    "tf_illegal_position_masks = tf.placeholder(tf.float32,[None,None,None])\n",
    "\n",
    "tf_pe_out = tf.placeholder(tf.float32,[None,None,None]) #positional codes for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Dimensions for Q (Query),K (Keys) and V (Values) for attention layers.\n",
    "\n",
    "dqkv = 32 \n",
    "    \n",
    "#Parameters for attention sub-layers for all n encoders\n",
    "\n",
    "Wq_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wk_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wv_enc = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wo_enc = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
    "\n",
    "#Parameters for position-wise fully connected layers for n encoders\n",
    "\n",
    "d = 1024\n",
    "W1_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01))\n",
    "b1_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,d]))\n",
    "W2_enc = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01))\n",
    "b2_enc = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim]))\n",
    "    \n",
    "#Parameters for 2 attention sub-layers for all n decoders\n",
    "\n",
    "Wq_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wk_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wv_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wo_dec_1 = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
    "Wq_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wk_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wv_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h,word_vec_dim,dqkv],stddev=0.01))\n",
    "Wo_dec_2 = tf.Variable(tf.truncated_normal(shape=[N,h*dqkv,word_vec_dim],stddev=0.01))\n",
    "    \n",
    "#Parameters for position-wise fully connected layers for n decoders\n",
    "\n",
    "d = 1024\n",
    "W1_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,word_vec_dim,d],stddev=0.01))\n",
    "b1_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,d]))\n",
    "W2_dec = tf.Variable(tf.truncated_normal(shape=[N,1,1,d,word_vec_dim],stddev=0.01))\n",
    "b2_dec = tf.Variable(tf.constant(0,tf.float32,shape=[N,word_vec_dim]))\n",
    "    \n",
    "#Layer Normalization parameters for encoder and decoder   \n",
    "\n",
    "scale_enc_1 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "shift_enc_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "\n",
    "scale_enc_2 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "shift_enc_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "\n",
    "#Layer Normalization parameters for decoder   \n",
    "\n",
    "scale_dec_1 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "shift_dec_1 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "\n",
    "scale_dec_2 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "shift_dec_2 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "\n",
    "scale_dec_3 = tf.Variable(tf.ones([N,1,1,word_vec_dim]),dtype=tf.float32)\n",
    "shift_dec_3 = tf.Variable(tf.zeros([N,1,1,word_vec_dim]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len,model_dimensions):\n",
    "    pe = np.zeros((seq_len,model_dimensions,),np.float32)\n",
    "    for pos in xrange(0,seq_len):\n",
    "        for i in xrange(0,model_dimensions):\n",
    "            pe[pos][i] = math.sin(pos/(10000**(2*i/model_dimensions)))\n",
    "    return pe.reshape((seq_len,model_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def layer_norm(inputs,scale,shift,epsilon = 1e-9):\n",
    "\n",
    "    mean, var = tf.nn.moments(inputs, [1,2], keep_dims=True)\n",
    "\n",
    "    LN = tf.multiply((scale / tf.sqrt(var + epsilon)),(inputs - mean)) + shift\n",
    " \n",
    "    return LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks_for_illegal_positions(out_len):\n",
    "    \n",
    "    masks=np.zeros((out_len-1,out_len,out_len),dtype=np.float32)\n",
    "    \n",
    "    for i in xrange(1,out_len):\n",
    "        mask = np.zeros((out_len,out_len),dtype=np.float32)\n",
    "        mask[i:out_len,:] = -2**30\n",
    "        mask[:,i:out_len] = -2**30\n",
    "        masks[i-1] = mask\n",
    "        \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def attention(Q,K,V,d,filled=0,mask=False):\n",
    "\n",
    "    K = tf.transpose(K,[0,2,1])\n",
    "    d = tf.cast(d,tf.float32)\n",
    "    \n",
    "    softmax_component = tf.div(tf.matmul(Q,K),tf.sqrt(d))\n",
    "    \n",
    "    if mask == True:\n",
    "        softmax_component = softmax_component + tf_illegal_position_masks[filled-1]\n",
    "        \n",
    "    result = tf.matmul(tf.nn.dropout(tf.nn.softmax(softmax_component),keep_prob),V)\n",
    " \n",
    "    return result\n",
    "       \n",
    "\n",
    "def multihead_attention(Q,K,V,d,weights,filled=0,mask=False):\n",
    "    \n",
    "    Q_ = tf.reshape(Q,[-1,tf.shape(Q)[2]])\n",
    "    K_ = tf.reshape(K,[-1,tf.shape(Q)[2]])\n",
    "    V_ = tf.reshape(V,[-1,tf.shape(Q)[2]])\n",
    "\n",
    "    heads = tf.TensorArray(size=h,dtype=tf.float32)\n",
    "    \n",
    "    Wq = weights['Wq']\n",
    "    Wk = weights['Wk']\n",
    "    Wv = weights['Wv']\n",
    "    Wo = weights['Wo']\n",
    "    \n",
    "    for i in xrange(0,h):\n",
    "        \n",
    "        Q_w = tf.matmul(Q_,Wq[i])\n",
    "        Q_w = tf.reshape(Q_w,[tf.shape(Q)[0],tf.shape(Q)[1],d])\n",
    "        \n",
    "        K_w = tf.matmul(K_,Wk[i])\n",
    "        K_w = tf.reshape(K_w,[tf.shape(K)[0],tf.shape(K)[1],d])\n",
    "        \n",
    "        V_w = tf.matmul(V_,Wv[i])\n",
    "        V_w = tf.reshape(V_w,[tf.shape(V)[0],tf.shape(V)[1],d])\n",
    "\n",
    "        head = attention(Q_w,K_w,V_w,d,filled,mask)\n",
    "            \n",
    "        heads = heads.write(i,head)\n",
    "        \n",
    "    heads = heads.stack()\n",
    "    \n",
    "    concated = heads[0]\n",
    "    \n",
    "    for i in xrange(1,h):\n",
    "        concated = tf.concat([concated,heads[i]],2)\n",
    "\n",
    "    concated = tf.reshape(concated,[-1,h*d])\n",
    "    out = tf.matmul(concated,Wo)\n",
    "    out = tf.reshape(out,[tf.shape(heads)[1],tf.shape(heads)[2],word_vec_dim])\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x,weights,attention_weights,dqkv):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    W2 = weights['W2']\n",
    "    b1 = weights['b1']\n",
    "    b2 = weights['b2']\n",
    "    \n",
    "    scale1 = weights['scale1']\n",
    "    shift1 = weights['shift1']\n",
    "    scale2 = weights['scale2']\n",
    "    shift2 = weights['shift2']\n",
    "    \n",
    "    # SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION)\n",
    "    \n",
    "    sublayer1 = multihead_attention(x,x,x,dqkv,attention_weights)\n",
    "    sublayer1 = tf.nn.dropout(sublayer1,keep_prob)\n",
    "    sublayer1 = layer_norm(sublayer1 + x,scale1,shift1)\n",
    "    \n",
    "    sublayer1_ = tf.reshape(sublayer1,[tf.shape(sublayer1)[0],1,tf.shape(sublayer1)[1],word_vec_dim])\n",
    "    \n",
    "    # SUBLAYER 2 (TWO 1x1 CONVOLUTIONAL LAYERS AKA POSITION WISE FULLY CONNECTED NETWORKS)\n",
    "    \n",
    "    sublayer2 = tf.nn.conv2d(sublayer1_, W1, strides=[1,1,1,1], padding='SAME')\n",
    "    sublayer2 = tf.nn.bias_add(sublayer2,b1)\n",
    "    sublayer2 = tf.nn.relu(sublayer2)\n",
    "    \n",
    "    sublayer2 = tf.nn.conv2d(sublayer2, W2, strides=[1,1,1,1], padding='SAME')\n",
    "    sublayer2 = tf.nn.bias_add(sublayer2,b2)\n",
    "    \n",
    "    sublayer2 = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],tf.shape(sublayer2)[2],word_vec_dim])\n",
    "    \n",
    "    sublayer2 = tf.nn.dropout(sublayer2,keep_prob)\n",
    "    sublayer2 = layer_norm(sublayer2 + sublayer1,scale2,shift2)\n",
    "    \n",
    "    return sublayer2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(y,enc_out,weights,masked_attention_weights,attention_weights,dqkv,mask=False,filled=0):\n",
    "\n",
    "    W1 = weights['W1']\n",
    "    W2 = weights['W2']\n",
    "    b1 = weights['b1']\n",
    "    b2 = weights['b2']\n",
    "    \n",
    "    scale1 = weights['scale1']\n",
    "    shift1 = weights['shift1']\n",
    "    scale2 = weights['scale2']\n",
    "    shift2 = weights['shift2']\n",
    "    scale3 = weights['scale3']\n",
    "    shift3 = weights['shift3']\n",
    "    \n",
    "    # SUBLAYER 1 (MASKED MULTI HEADED SELF ATTENTION)\n",
    "\n",
    "    sublayer1 = multihead_attention(y,y,y,dqkv,masked_attention_weights,filled,mask)\n",
    "    sublayer1 = tf.nn.dropout(sublayer1,keep_prob)\n",
    "    sublayer1 = layer_norm(sublayer1 + y,scale1,shift1)\n",
    "    \n",
    "    # SUBLAYER 2 (MULTIHEADED ENCODER-DECODER INTERLAYER ATTENTION)\n",
    "    \n",
    "    sublayer2 = multihead_attention(sublayer1,enc_out,enc_out,dqkv,attention_weights)\n",
    "    sublayer2 = tf.nn.dropout(sublayer2,keep_prob)\n",
    "    sublayer2 = layer_norm(sublayer2 + sublayer1,scale2,shift2)\n",
    "    \n",
    "    # SUBLAYER 3 (TWO 1x1 CONVOLUTIONAL LAYERS AKA POSITION WISE FULLY CONNECTED NETWORKS)\n",
    "    \n",
    "    sublayer2_ = tf.reshape(sublayer2,[tf.shape(sublayer2)[0],1,tf.shape(sublayer2)[1],word_vec_dim])\n",
    "    \n",
    "    sublayer3 = tf.nn.conv2d(sublayer2_, W1, strides=[1,1,1,1], padding='SAME')\n",
    "    sublayer3 = tf.nn.bias_add(sublayer3,b1)\n",
    "    sublayer3 = tf.nn.relu(sublayer3)\n",
    "    \n",
    "    sublayer3 = tf.nn.conv2d(sublayer3, W2, strides=[1,1,1,1], padding='SAME')\n",
    "    sublayer3 = tf.nn.bias_add(sublayer3,b2)\n",
    "    \n",
    "    sublayer3 = tf.reshape(sublayer3,[tf.shape(sublayer3)[0],tf.shape(sublayer3)[2],word_vec_dim])\n",
    "    \n",
    "    sublayer3 = tf.nn.dropout(sublayer3,keep_prob)\n",
    "    sublayer3 = layer_norm(sublayer3 + sublayer2,scale3,shift3)\n",
    "    \n",
    "    return sublayer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_encoders(layer_num,encoderin):\n",
    "    \n",
    "    for i in xrange(0,layer_num):\n",
    "        \n",
    "        encoder_weights = {\n",
    "            \n",
    "            'W1': W1_enc[i],\n",
    "            'W2': W2_enc[i],\n",
    "            'b1': b1_enc[i],\n",
    "            'b2': b2_enc[i],\n",
    "            'scale1': scale_enc_1[i],\n",
    "            'shift1': shift_enc_1[i],\n",
    "            'scale2': scale_enc_2[i],\n",
    "            'shift2': shift_enc_2[i],\n",
    "        }\n",
    "        \n",
    "        attention_weights = {\n",
    "            \n",
    "            'Wq': Wq_enc[i],\n",
    "            'Wk': Wk_enc[i],\n",
    "            'Wv': Wv_enc[i],\n",
    "            'Wo': Wo_enc[i],                       \n",
    "        }\n",
    "        \n",
    "        encoderin = encoder(encoderin,encoder_weights,attention_weights,dqkv)\n",
    "    \n",
    "    return encoderin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_decoders(layer_num,decoderin,encoderout,filled):\n",
    "    \n",
    "    for j in xrange(0,layer_num):\n",
    "        \n",
    "        decoder_weights = {\n",
    "            \n",
    "            'W1': W1_dec[j],\n",
    "            'W2': W2_dec[j],\n",
    "            'b1': b1_dec[j],\n",
    "            'b2': b2_dec[j],\n",
    "            'scale1': scale_dec_1[j],\n",
    "            'shift1': shift_dec_1[j],\n",
    "            'scale2': scale_dec_2[j],\n",
    "            'shift2': shift_dec_2[j],\n",
    "            'scale3': scale_dec_3[j],\n",
    "            'shift3': shift_dec_3[j],\n",
    "        }\n",
    "            \n",
    "        masked_attention_weights = {\n",
    "            \n",
    "            'Wq': Wq_dec_1[j],\n",
    "            'Wk': Wk_dec_1[j],\n",
    "            'Wv': Wv_dec_1[j],\n",
    "            'Wo': Wo_dec_1[j],                       \n",
    "        }\n",
    "        \n",
    "        attention_weights = {\n",
    "            \n",
    "            'Wq': Wq_dec_2[j],\n",
    "            'Wk': Wk_dec_2[j],\n",
    "            'Wv': Wv_dec_2[j],\n",
    "            'Wo': Wo_dec_2[j],                       \n",
    "        }\n",
    "            \n",
    "        decoderin = decoder(decoderin,encoderout,\n",
    "                            decoder_weights,\n",
    "                            masked_attention_weights,\n",
    "                            attention_weights,\n",
    "                            dqkv,\n",
    "                            mask=True,filled=filled)\n",
    "    return decoderin\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_embedding(out_prob_dist,tf_embd):\n",
    "    out_index = tf.cast(tf.argmax(out_prob_dist,1),tf.int32)\n",
    "    return tf.gather(tf_embd,out_index)\n",
    "\n",
    "def replaceSOS(output,out_prob_dist):\n",
    "    return output,tf.constant(1),tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len])\n",
    "\n",
    "def add_pred_to_output_list(decoderin_part_1,output,filled,out_probs,out_prob_dist):\n",
    "    decoderin_part_1 = tf.concat([decoderin_part_1,output],1)\n",
    "    filled += 1\n",
    "    out_probs = tf.concat([out_probs,tf.reshape(out_prob_dist,[tf.shape(x)[0],1,vocab_len])],1)\n",
    "    return decoderin_part_1,filled,out_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,teacher_forcing=True):\n",
    "    \n",
    "        \n",
    "    # NOTE: tf.shape(x)[0] == batch_size\n",
    "    \n",
    "    encoderin = x # (should be already positionally encoded) \n",
    "    encoderin = tf.nn.dropout(encoderin,keep_prob)\n",
    "\n",
    "    \n",
    "    # ENCODER LAYERS\n",
    "\n",
    "    encoderout = stacked_encoders(N,encoderin)\n",
    "    \n",
    "\n",
    "    # DECODER LAYERS\n",
    "\n",
    "    decoderin_part_1 = tf.ones([tf.shape(x)[0],1,word_vec_dim],dtype=tf.float32) #represents SOS\n",
    "    \n",
    "    filled = tf.constant(1) \n",
    "    # no. of output words that are filled\n",
    "    # filled value is used to retrieve appropriate mask for illegal positions. \n",
    "    \n",
    "    \n",
    "    tf_embd = tf.convert_to_tensor(np_embd_limit)\n",
    "    Wpd = tf.transpose(tf_embd)\n",
    "    # Wpd the transpose of the output embedding matrix will be used to convert the decoder output\n",
    "    # into a probability distribution over the output language vocabulary. \n",
    "    \n",
    "    out_probs = tf.zeros([tf.shape(x)[0],output_len,vocab_len],tf.float32)\n",
    "    # out_probs will contain the list of probability distributions.\n",
    "\n",
    "    #tf_while_loop since output_len will be dynamically defined during session run\n",
    "    \n",
    "    i=tf.constant(0)\n",
    "    \n",
    "    def cond(i,filled,decoderin_part_1,out_probs):\n",
    "        return i<output_len\n",
    "    \n",
    "    def body(i,filled,decoderin_part_1,out_probs):\n",
    "        \n",
    "        decoderin_part_2 = tf.zeros([tf.shape(x)[0],(output_len-filled),word_vec_dim],dtype=tf.float32)\n",
    "        \n",
    "        decoderin = tf.concat([decoderin_part_1,decoderin_part_2],1)\n",
    "        \n",
    "        decoderin = tf.nn.dropout(decoderin,keep_prob)\n",
    "        \n",
    "        decoderout = stacked_decoders(N,decoderin,encoderout,filled)\n",
    "        \n",
    "        # decoderout shape (now) = batch_size x seq_len x word_vec_dim\n",
    "\n",
    "        decoderout = tf.reduce_sum(decoderout,1) \n",
    "        # A weighted summation of the attended decoder input\n",
    "        # decoderout shape (now) = batch_size x word_vec_dim\n",
    "        \n",
    "        # converting decoderout to probability distributions\n",
    "        \n",
    "        out_prob_dist = tf.matmul(decoderout,Wpd)\n",
    "   \n",
    "        # If teacher forcing is false, initiate predicted_embedding(). It guesses the output embeddings\n",
    "        # to be that whose vocabulary index has maximum probability in out_prob_dist\n",
    "        # (the current output probability distribution). The embedding is used in the next\n",
    "        # iteration. \n",
    "        \n",
    "        # If teacher forcing is true, use the embedding of target index from y (laebls) \n",
    "        # for the next iteration.\n",
    "        \n",
    "        output = tf.cond(tf.equal(teacher_forcing,tf.convert_to_tensor(False)),\n",
    "                         lambda: predicted_embedding(out_prob_dist,tf_embd),\n",
    "                         lambda: tf.gather(tf_embd,y[:,i]))\n",
    "        \n",
    "        # Position Encoding the output\n",
    "        \n",
    "        output = output + tf_pe_out[i]\n",
    "        output = tf.reshape(output,[tf.shape(x)[0],1,word_vec_dim])\n",
    "                                \n",
    "        \n",
    "        #concatenate with list of previous predicted output tokens\n",
    "        \n",
    "        decoderin_part_1,filled,out_probs = tf.cond(tf.equal(i,0),\n",
    "                                        lambda:replaceSOS(output,out_prob_dist),\n",
    "                                        lambda:add_pred_to_output_list(decoderin_part_1,output,filled,out_probs,out_prob_dist))\n",
    "        \n",
    "        return i+1,filled,decoderin_part_1,out_probs\n",
    "            \n",
    "    _,_,_,out_probs = tf.while_loop(cond,body,[i,filled,decoderin_part_1,out_probs],\n",
    "                      shape_invariants=[i.get_shape(),\n",
    "                                        filled.get_shape(),\n",
    "                                        tf.TensorShape([None,None,word_vec_dim]),\n",
    "                                        tf.TensorShape([None,None,vocab_len])])\n",
    "\n",
    "    return out_probs          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Model\n",
    "output = model(x,teacher_forcing)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=y)\n",
    "cost = tf.multiply(cost,tf_pad_mask) #mask used to remove loss effect due to PADS\n",
    "cost = tf.reduce_mean(cost)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,beta1=0.9,beta2=0.98,epsilon=1e-9).minimize(cost)\n",
    "\n",
    "#wanna add some temperature?\n",
    "\n",
    "\"\"\"temperature = 0.7\n",
    "scaled_output = tf.log(output)/temperature\n",
    "softmax_output = tf.nn.softmax(scaled_output)\"\"\"\n",
    "\n",
    "#(^Use it with \"#prediction_int = np.random.choice(range(vocab_len), p=array.ravel())\")\n",
    "\n",
    "softmax_output = tf.nn.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_out(output_batch):\n",
    "    out = []\n",
    "    for output_text in output_batch:\n",
    "        output_len = len(output_text)\n",
    "        transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "        for i in xrange(0,output_len):\n",
    "            transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "        out.append(transformed_output)\n",
    "    return np.asarray(out,np.int32)\n",
    "\n",
    "def create_pad_Mask(output_batch):\n",
    "    pad_index = vocab_limit.index('<PAD>')\n",
    "    mask = np.ones_like((output_batch),np.float32)\n",
    "    for i in xrange(len(mask)):\n",
    "        for j in xrange(len(mask[i])):\n",
    "            if output_batch[i,j]==pad_index:\n",
    "                mask[i,j]=0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHOSEN SAMPLE NO.: 14\n",
      "\n",
      "Epoch: 1 Iteration: 1\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this coffee has an excellent flavor we love our keurig <UNK> brewer <PAD> <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "percent a of our their\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "one of our favorites <EOS> \n",
      "\n",
      "loss=34.637\n",
      "\n",
      "CHOSEN SAMPLE NO.: 43\n",
      "\n",
      "Epoch: 1 Iteration: 2\n",
      "\n",
      "SAMPLE TEXT:\n",
      "excellent tea as usual and the delivery service was outstanding so no worries buyer \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "cardiomyopathy tremendous immense absolute god\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "immense pleasure <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=31.1455\n",
      "\n",
      "CHOSEN SAMPLE NO.: 0\n",
      "\n",
      "Epoch: 1 Iteration: 3\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is an excellent tea subtle pear taste in an exquisite white tea blend <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "stadium beautiful beautiful flavors flavor\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "fabulous subtle pear taste <EOS> \n",
      "\n",
      "loss=29.9571\n",
      "\n",
      "CHOSEN SAMPLE NO.: 37\n",
      "\n",
      "Epoch: 1 Iteration: 4\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is my <UNK> favorite hot cocoa brand the kids favorite flavor if mint chocolate \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "humor the world world rock\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "the best hot chocolate <EOS> \n",
      "\n",
      "loss=27.0493\n",
      "\n",
      "CHOSEN SAMPLE NO.: 12\n",
      "\n",
      "Epoch: 1 Iteration: 5\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is the best tea i like it as well as coffee highly recommend it \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "<EOS> the world world music\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "the best tea delicious <EOS> \n",
      "\n",
      "loss=25.4263\n",
      "\n",
      "CHOSEN SAMPLE NO.: 5\n",
      "\n",
      "Epoch: 1 Iteration: 6\n",
      "\n",
      "SAMPLE TEXT:\n",
      "crispy with the light flavor of rice and sesame lightly salted i love these crackers <PAD> \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "<EOS> flavored <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "delicious <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=20.3154\n",
      "\n",
      "CHOSEN SAMPLE NO.: 34\n",
      "\n",
      "Epoch: 1 Iteration: 7\n",
      "\n",
      "SAMPLE TEXT:\n",
      "very good as a hot cereal or as coating for fried chicken my family loves it \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "<EOS> <EOS> you <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good stuff <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=16.609\n",
      "\n",
      "CHOSEN SAMPLE NO.: 40\n",
      "\n",
      "Epoch: 1 Iteration: 8\n",
      "\n",
      "SAMPLE TEXT:\n",
      "loved this tea however make sure you brew it long enough per <UNK> instructions on box \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "charm <EOS> <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "great teabag tea <EOS> <PAD> \n",
      "\n",
      "loss=15.9044\n",
      "\n",
      "CHOSEN SAMPLE NO.: 22\n",
      "\n",
      "Epoch: 1 Iteration: 9\n",
      "\n",
      "SAMPLE TEXT:\n",
      "being what you would call a chip connoisseur was looking forward to trying these great chips \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "good <EOS> jalapeno chips <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "best jalapeno chips <EOS> <PAD> \n",
      "\n",
      "loss=17.7153\n",
      "\n",
      "CHOSEN SAMPLE NO.: 28\n",
      "\n",
      "Epoch: 1 Iteration: 10\n",
      "\n",
      "SAMPLE TEXT:\n",
      "<UNK> is so good it really tastes like bacon just creamier i would highly recommend this product \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "wonderful yum <EOS> <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "yum <EOS> <PAD> <PAD> <PAD> \n",
      "\n",
      "loss=15.4829\n",
      "\n",
      "CHOSEN SAMPLE NO.: 33\n",
      "\n",
      "Epoch: 1 Iteration: 11\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this is a great mix i mixed it with milk instead of water just follow the directions \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "sweet cuisine flavor <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "tasty texture great <EOS> <PAD> \n",
      "\n",
      "loss=15.3569\n",
      "\n",
      "CHOSEN SAMPLE NO.: 22\n",
      "\n",
      "Epoch: 1 Iteration: 12\n",
      "\n",
      "SAMPLE TEXT:\n",
      "this kettle chips taste good crispy crunchy too u will enjoy it also <UNK> thinly cut sliced \n",
      "\n",
      "\n",
      "PREDICTED SUMMARY OF THE SAMPLE:\n",
      "\n",
      "sweet kettle chips <EOS> <EOS>\n",
      "\n",
      "ACTUAL SUMMARY OF THE SAMPLE:\n",
      "\n",
      "kettle chips <EOS> <PAD> <PAD> \n",
      "\n",
      "loss=16.5869\n",
      "\n",
      "CHOSEN SAMPLE NO.: 43\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    best_loss = 999\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < epochs:\n",
    "           \n",
    "        batch_len = len(train_batches_x_pe)\n",
    "        for i in xrange(0,batch_len):\n",
    "            \n",
    "            sample_no = np.random.randint(0,batch_size)\n",
    "            print(\"\\nCHOSEN SAMPLE NO.: \"+str(sample_no))\n",
    "            \n",
    "            train_out = transform_out(train_batches_y[i])\n",
    "\n",
    "            if i%display_step==0:\n",
    "                print(\"\\nEpoch: \"+str(step+1)+\" Iteration: \"+str(i+1))\n",
    "                print(\"\\nSAMPLE TEXT:\")\n",
    "                for vec in train_batches_x[i][sample_no]:\n",
    "                    print(str(vec2word(vec)),end=\" \")\n",
    "                print(\"\\n\")\n",
    "                \n",
    "            rand = random.randint(0,4) #determines chance of using Teacher Forcing\n",
    "            if rand==2:\n",
    "                random_bool = False\n",
    "            else:\n",
    "                random_bool = True\n",
    "                \n",
    "            output_seq_len = len(train_out[0])\n",
    "            \n",
    "            illegal_position_masks = generate_masks_for_illegal_positions(output_seq_len)\n",
    "            \n",
    "            pe_out = positional_encoding(output_seq_len,word_vec_dim)\n",
    "            pe_out = pe_out.reshape((output_seq_len,1,word_vec_dim))\n",
    "            \n",
    "            pad_mask = create_pad_Mask(train_out)\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,out = sess.run([optimizer,cost,softmax_output],feed_dict={x: train_batches_x_pe[i], \n",
    "                                                                             y: train_out,\n",
    "                                                                             keep_prob: 0.9,\n",
    "                                                                             output_len: output_seq_len,\n",
    "                                                                             tf_illegal_position_masks: illegal_position_masks,\n",
    "                                                                             tf_pe_out: pe_out,\n",
    "                                                                             tf_pad_mask: pad_mask,\n",
    "                                                                             teacher_forcing: random_bool})\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY OF THE SAMPLE:\\n\")\n",
    "                flag = 0\n",
    "                for array in out[sample_no]:\n",
    "                    \n",
    "                    #prediction_int = np.random.choice(range(vocab_len), p=array.ravel()) \n",
    "                    #(^use this if you want some variety)\n",
    "                    #(or use this what's below:)\n",
    "                    \n",
    "                    prediction_int = np.argmax(array)\n",
    "                    \n",
    "                    if vocab_limit[prediction_int] in string.punctuation or flag==0: \n",
    "                        print(str(vocab_limit[prediction_int]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[prediction_int]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY OF THE SAMPLE:\\n\")\n",
    "                for vec in batches_y[i][sample_no]:\n",
    "                    print(str(vec2word(vec)),end=\" \")\n",
    "                print(\"\\n\")\n",
    "            \n",
    "            print(\"loss=\"+str(loss))\n",
    "                  \n",
    "            if(loss<best_loss):\n",
    "                best_loss = loss\n",
    "                saver.save(sess, 'Model_Backup/allattmodel.ckpt')\n",
    "\n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

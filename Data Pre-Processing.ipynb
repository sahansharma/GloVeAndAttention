{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector embeddings Loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "# Constants\n",
    "EMBEDDING_FILE = 'glove.6B.50d.txt'\n",
    "UNKNOWN_TOKEN = '<UNK>'\n",
    "END_TOKEN = '<EOS>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "EPSILON = 0.0001\n",
    "\n",
    "def load_word_embeddings(file_path):\n",
    "    \"\"\"Load pre-trained word embeddings from file.\"\"\"\n",
    "    vocabulary = []\n",
    "    embeddings = []\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            vocabulary.append(parts[0])\n",
    "            embeddings.append(parts[1:])\n",
    "    \n",
    "    print('Word vector embeddings loaded successfully.')\n",
    "    return vocabulary, embeddings\n",
    "\n",
    "# Load pre-trained word embeddings\n",
    "vocabulary, embeddings = load_word_embeddings(EMBEDDING_FILE)\n",
    "embedding_dim = len(embeddings[0])  # Dimension of each word vector\n",
    "\n",
    "# Prepare special tokens\n",
    "small_value = np.zeros((embedding_dim,), dtype=np.float32) + EPSILON\n",
    "\n",
    "# Add unknown token\n",
    "vocabulary.append(UNKNOWN_TOKEN)\n",
    "unk_embedding = np.asarray(embeddings[vocabulary.index('unk')], dtype=np.float32) + small_value\n",
    "\n",
    "# Add end-of-sentence token\n",
    "vocabulary.append(END_TOKEN)\n",
    "eos_embedding = np.asarray(embeddings[vocabulary.index('eos')], dtype=np.float32) + small_value\n",
    "\n",
    "# Add padding token\n",
    "vocabulary.append(PAD_TOKEN)\n",
    "\n",
    "# Check if special tokens already exist in embeddings\n",
    "has_unk = False\n",
    "has_eos = False\n",
    "\n",
    "for vector in embeddings:\n",
    "    if np.all(np.equal(np.asarray(vector, dtype=np.float32), unk_embedding)):\n",
    "        has_unk = True\n",
    "        print(\"UNK token already exists in embeddings\")\n",
    "    if np.all(np.equal(np.asarray(vector, dtype=np.float32), eos_embedding)):\n",
    "        has_eos = True\n",
    "        print(\"EOS token already exists in embeddings\")\n",
    "\n",
    "# Add special token embeddings if they don't exist\n",
    "if not has_unk:\n",
    "    embeddings.append(unk_embedding)\n",
    "if not has_eos:\n",
    "    embeddings.append(eos_embedding)\n",
    "\n",
    "# Add zero vector for padding\n",
    "pad_embedding = np.zeros(embedding_dim, dtype=np.float32)\n",
    "embeddings.append(pad_embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embedding_matrix = np.asarray(embeddings, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_vector(word, vocabulary, embedding_matrix):\n",
    "    \"\"\"Convert a word to its vector representation.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word to convert to vector\n",
    "        vocabulary (list): List of words in the vocabulary\n",
    "        embedding_matrix (np.ndarray): Matrix containing word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Vector representation of the word\n",
    "    \"\"\"\n",
    "    if word in vocabulary:\n",
    "        return embedding_matrix[vocabulary.index(word)]\n",
    "    return embedding_matrix[vocabulary.index(UNKNOWN_TOKEN)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP TEN MOST SIMILAR WORDS TO 'frog':\n",
      "\n",
      "1. frog\n",
      "2. snake\n",
      "3. ape\n",
      "4. toad\n",
      "5. monkey\n",
      "6. spider\n",
      "7. lizard\n",
      "8. tarantula\n",
      "9. cat\n",
      "10. spiny\n"
     ]
    }
   ],
   "source": [
    "def find_most_similar_by_cosine(query_vector, embedding_matrix):\n",
    "    \"\"\"Find most similar words using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        query_vector (np.ndarray): The vector to compare against\n",
    "        embedding_matrix (np.ndarray): Matrix containing all word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Indices of words sorted by cosine similarity (descending)\n",
    "    \"\"\"\n",
    "    # Compute dot products\n",
    "    dot_products = np.sum(np.multiply(embedding_matrix, query_vector), axis=1)\n",
    "    \n",
    "    # Compute vector magnitudes\n",
    "    query_norm = np.sqrt(np.sum(np.square(query_vector)))\n",
    "    embedding_norms = np.sqrt(np.sum(np.square(embedding_matrix), axis=1))\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    cosine_similarities = np.divide(dot_products, \n",
    "                                  np.multiply(query_norm, embedding_norms))\n",
    "    \n",
    "    # Return indices sorted by similarity (highest first)\n",
    "    return np.flip(np.argsort(cosine_similarities), axis=0)\n",
    "\n",
    "\n",
    "def find_most_similar_by_euclidean(query_vector, embedding_matrix):\n",
    "    \"\"\"Find most similar words using Euclidean distance.\n",
    "    \n",
    "    Args:\n",
    "        query_vector (np.ndarray): The vector to compare against\n",
    "        embedding_matrix (np.ndarray): Matrix containing all word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Indices of words sorted by Euclidean distance (ascending)\n",
    "    \"\"\"\n",
    "    differences = np.subtract(embedding_matrix, query_vector)\n",
    "    squared_differences = np.square(differences)\n",
    "    sum_squares = np.sum(squared_differences, axis=1)\n",
    "    euclidean_distances = np.sqrt(sum_squares)\n",
    "    return np.argsort(euclidean_distances)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "target_word = 'frog'\n",
    "top_n = 10\n",
    "\n",
    "# Get vector for target word\n",
    "word_vector = word_to_vector(target_word, vocabulary, embedding_matrix)\n",
    "\n",
    "# Find most similar words\n",
    "similar_indices = find_most_similar_by_euclidean(word_vector, embedding_matrix)\n",
    "\n",
    "# Display results\n",
    "print(f\"TOP {top_n} MOST SIMILAR WORDS TO '{target_word}':\\n\")\n",
    "for rank, index in enumerate(similar_indices[:top_n], start=1):\n",
    "    print(f\"{rank}. {vocabulary[index]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_to_word(query_vector, vocabulary, embedding_matrix):\n",
    "    \"\"\"Convert a vector to its most similar word in the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        query_vector (np.ndarray): The vector representation to convert\n",
    "        vocabulary (list): List of words in the vocabulary\n",
    "        embedding_matrix (np.ndarray): Matrix containing word embeddings\n",
    "        \n",
    "    Returns:\n",
    "        str: The most similar word in the vocabulary\n",
    "        \n",
    "    Note:\n",
    "        Uses Euclidean distance to find the closest word embedding\n",
    "    \"\"\"\n",
    "    # Ensure input is properly formatted as numpy array\n",
    "    query_vector = np.asarray(query_vector, dtype=np.float32)\n",
    "    \n",
    "    # Find indices of most similar words (sorted by ascending Euclidean distance)\n",
    "    similar_word_indices = find_most_similar_by_euclidean(query_vector, embedding_matrix)\n",
    "    \n",
    "    # Return the closest matching word\n",
    "    return vocabulary[similar_word_indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Constants\n",
    "MAX_DATA_POINTS = 100000\n",
    "CSV_FILE_PATH = 'Reviews.csv'\n",
    "TEXT_COLUMN = 'Text'\n",
    "SUMMARY_COLUMN = 'Summary'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing non-printable characters\n",
    "    3. Removing punctuation\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Remove non-printable characters\n",
    "    printable_chars = set(string.printable)\n",
    "    text = ''.join(filter(lambda x: x in printable_chars, text))\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def load_review_data(file_path, max_samples):\n",
    "    \"\"\"Load and preprocess review data from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to CSV file\n",
    "        max_samples (int): Maximum number of samples to load\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list of tokenized texts, list of tokenized summaries)\n",
    "    \"\"\"\n",
    "    tokenized_texts = []\n",
    "    tokenized_summaries = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for count, row in enumerate(reader):\n",
    "            if count >= max_samples:\n",
    "                break\n",
    "                \n",
    "            cleaned_text = clean_text(row[TEXT_COLUMN])\n",
    "            cleaned_summary = clean_text(row[SUMMARY_COLUMN])\n",
    "            \n",
    "            tokenized_texts.append(word_tokenize(cleaned_text))\n",
    "            tokenized_summaries.append(word_tokenize(cleaned_summary))\n",
    "            \n",
    "    return tokenized_texts, tokenized_summaries\n",
    "\n",
    "# Load and preprocess the data\n",
    "texts, summaries = load_review_data(CSV_FILE_PATH, MAX_DATA_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 48478\n"
     ]
    }
   ],
   "source": [
    "# Configuration constants\n",
    "MAX_TEXT_LENGTH = 80\n",
    "MAX_SUMMARY_LENGTH = 4\n",
    "\n",
    "def filter_data_by_length(texts, summaries, max_text_len, max_summary_len):\n",
    "    \"\"\"Filter text-summary pairs based on length constraints.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of tokenized texts\n",
    "        summaries (list): List of tokenized summaries\n",
    "        max_text_len (int): Maximum allowed text length\n",
    "        max_summary_len (int): Maximum allowed summary length\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filtered_texts, filtered_summaries)\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    filtered_summaries = []\n",
    "    \n",
    "    for text, summary in zip(texts, summaries):\n",
    "        if len(text) <= max_text_len and len(summary) <= max_summary_len:\n",
    "            filtered_texts.append(text)\n",
    "            filtered_summaries.append(summary)\n",
    "    \n",
    "    return filtered_texts, filtered_summaries\n",
    "\n",
    "# Filter the data\n",
    "filtered_texts, filtered_summaries = filter_data_by_length(\n",
    "    texts, summaries, MAX_TEXT_LENGTH, MAX_SUMMARY_LENGTH\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(f\"Current size of filtered data: {len(filtered_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current size of data: 44413\n"
     ]
    }
   ],
   "source": [
    "def filter_by_vocabulary(texts, summaries, vocabulary):\n",
    "    \"\"\"Filter text-summary pairs where all summary words are in vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of tokenized texts\n",
    "        summaries (list): List of tokenized summaries\n",
    "        vocabulary (list): List of known vocabulary words\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filtered_texts, filtered_summaries)\n",
    "    \"\"\"\n",
    "    filtered_texts = []\n",
    "    filtered_summaries = []\n",
    "    \n",
    "    vocabulary_set = set(vocabulary)  # Convert to set for faster lookups\n",
    "    \n",
    "    for text, summary in zip(texts, summaries):\n",
    "        # Check if all words in summary are in vocabulary\n",
    "        if all(word in vocabulary_set for word in summary):\n",
    "            filtered_summaries.append(summary)\n",
    "            filtered_texts.append(text)\n",
    "    \n",
    "    return filtered_texts, filtered_summaries\n",
    "\n",
    "# Filter the data\n",
    "filtered_texts_vocab, filtered_summaries_vocab = filter_by_vocabulary(\n",
    "    texts_v2, summaries_v2, vocab\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(f\"Current size of vocabulary-filtered data: {len(filtered_texts_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#REDUCE DATA (FOR SPEEDING UP THE NEXT STEPS)\n",
    "\n",
    "MAXIMUM_DATA_NUM = 20000\n",
    "\n",
    "texts = texts_v3[0:MAXIMUM_DATA_NUM]\n",
    "summaries = summaries_v3[0:MAXIMUM_DATA_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE CLEANED & TOKENIZED TEXT: \n",
      "\n",
      "['our', 'boston', 'terrier', 'loves', 'these', 'bones', 'we', 'give', 'them', 'to', 'her', 'as', 'a', 'treat', 'or', 'to', 'keep', 'her', 'busy', 'when', 'we', 'have', 'company', 'for', 'a', '16', 'lbs', 'dog', 'shes', 'a', 'mighty', 'chewer', 'and', 'these', 'last', 'her', 'a', 'couple', 'of', 'hours', 'with', 'breaks', 'to', 'investigate', 'if', 'shes', 'missing', 'anything', 'well', 'buy', 'more', 'of', 'these']\n",
      "\n",
      "SAMPLE CLEANED & TOKENIZED SUMMARY: \n",
      "\n",
      "['chloe', 'loves', 'them']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "index = random.randint(0,len(texts)-1)\n",
    "\n",
    "print \"SAMPLE CLEANED & TOKENIZED TEXT: \\n\\n\"+str(texts[index])\n",
    "print \"\\nSAMPLE CLEANED & TOKENIZED SUMMARY: \\n\\n\"+str(summaries[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit = []\n",
    "i=0\n",
    "for text in texts:\n",
    "    for word in text:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for summary in summaries:\n",
    "    for word in summary:\n",
    "        if word not in vocab_limit:\n",
    "            if word in vocab:\n",
    "                vocab_limit.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_limit.append('<EOS>')\n",
    "vocab_limit.append('<UNK>')\n",
    "vocab_limit.append('<PAD>') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lentexts = []\n",
    "\n",
    "i=0\n",
    "for text in texts:\n",
    "    lentexts.append(len(text))\n",
    "    i+=1\n",
    "    \n",
    "sortedindex = np.argsort(lentexts)\n",
    "#sort indexes according to the sequence length of corresponding texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "bi=0\n",
    "\n",
    "batches_x = []\n",
    "batches_y = []\n",
    "batch_x = []\n",
    "batch_y = []\n",
    "\n",
    "for i in xrange(0,len(texts)):\n",
    "    \n",
    "    if bi>=batch_size:\n",
    "        bi=0\n",
    "        batches_x.append(batch_x)\n",
    "        batches_y.append(batch_y)\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "    batch_x.append(texts[int(sortedindex[i])])\n",
    "    batch_y.append(summaries[int(sortedindex[i])])\n",
    "    \n",
    "    bi+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vec_batches_x = []\n",
    "vec_batches_x_pe = []\n",
    "\n",
    "for batch in batches_x:\n",
    " \n",
    "    max_len_x = len(batch[batch_size-1])\n",
    "    vec_texts = []\n",
    "    vec_texts_pe = []\n",
    "    \n",
    "    for text in batch:\n",
    "        \n",
    "        vec_text=[]\n",
    "        vec_text_pe = []\n",
    "    \n",
    "        pos=0\n",
    "        \n",
    "        for word in text:\n",
    "            \n",
    "            pe = np.zeros((word_vec_dim,),np.float32)\n",
    "            #positional encoding\n",
    "            \n",
    "            for i in xrange(0,word_vec_dim):\n",
    "                pe[i] = math.sin(pos/math.pow(10000,(2*i/word_vec_dim)))\n",
    "            \n",
    "            vec_text.append(word2vec(word))\n",
    "            \n",
    "            ped = np.asarray(word2vec(word),np.float32) + pe\n",
    "            \n",
    "            vec_text_pe.append(ped)\n",
    "            \n",
    "            pos=pos+1\n",
    "        \n",
    "        n = len(vec_text)\n",
    "        \n",
    "        while n<max_len_x:\n",
    "            \n",
    "            vec_text.append(word2vec('<PAD>'))\n",
    "            vec_text_pe.append(word2vec('<PAD>'))\n",
    "            n = len(vec_text)\n",
    "        \n",
    "        vec_texts.append(vec_text)\n",
    "        vec_texts_pe.append(vec_text_pe)\n",
    "    \n",
    "    vec_texts = np.asarray(vec_texts,np.float32)\n",
    "    vec_batches_x.append(vec_texts)\n",
    "    \n",
    "    vec_texts_pe = np.asarray(vec_texts_pe,np.float32)\n",
    "    vec_batches_x_pe.append(vec_texts_pe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_batches_y = []\n",
    "#vec_batches_y_pe = []\n",
    "\n",
    "#k=0\n",
    "for batch in batches_y:\n",
    "\n",
    "    max_len_y = max_len_sum+1\n",
    "    vec_summaries = []\n",
    "\n",
    "    for summary in batch:\n",
    "        \n",
    "        vec_summary=[]\n",
    "        for word in summary:\n",
    "            vec_summary.append(word2vec(word))\n",
    "        \n",
    "        vec_summary.append(word2vec('<EOS>'))\n",
    "        \n",
    "        n = len(vec_summary)\n",
    "\n",
    "        while n<max_len_y:\n",
    "            vec_summary.append(word2vec('<PAD>'))\n",
    "            n = len(vec_summary)\n",
    "        #print n\n",
    "        \n",
    "        vec_summaries.append(vec_summary)\n",
    "    \n",
    "    vec_summaries = np.asarray(vec_summaries,np.float32)\n",
    "    vec_batches_y.append(vec_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Saving processed data in another file.\n",
    "\n",
    "import pickle\n",
    "\n",
    "PICK = [vocab_limit,batch_size,vec_batches_x,vec_batches_y,vec_batches_x_pe,vec]\n",
    "\n",
    "with open('AmazonPICKLE', 'wb') as fp:\n",
    "    pickle.dump(PICK, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
